{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract text from multiple PDFs\n",
    "def extract_text_from_pdfs(pdf_folder):\n",
    "    text = \"\"\n",
    "    for file_name in os.listdir(pdf_folder):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, file_name)\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_database(chunks, embeddings_model):\n",
    "    save_path = r\"D:/AI-Projects/hydrogen-chatbot/vectore_db/vector_db.faiss\"\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading existing vector database from {save_path}...\")\n",
    "        vector_db = FAISS.load_local(save_path, HuggingFaceEmbeddings(model_name=embeddings_model),allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        if not chunks:\n",
    "            raise ValueError(\"Chunks are required to create a new vector database.\")\n",
    "        print(\"Creating a new vector database...\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=embeddings_model)\n",
    "        vector_db = FAISS.from_texts(chunks, embeddings)\n",
    "        vector_db.save_local(save_path)\n",
    "        print(f\"Vector database created and saved to {save_path}\")\n",
    "    \n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model_path):\n",
    "    llm = pipeline(\"question-answering\", \"timpal0l/mdeberta-v3-base-squad2\")\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chatbot(vector_db, llm):\n",
    "    retriever = vector_db.as_retriever()\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = \"D:\\AI-Projects\\hydrogen-chatbot\\data\"  # Folder containing multiple PDF files\n",
    "embeddings_model = \"sentence-transformers/all-MiniLM-L6-v2\"  # Open-source embeddings\n",
    "#pretrained_model_path = \"D:\\AI-Projects\\hydrogen-market-chatbot\\model\\pytorch_model.bin\"  # Path to the pre-trained Llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_text_from_pdfs(pdf_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks\n",
    "print(\"Splitting text into chunks...\")\n",
    "chunks = split_text_into_chunks(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = create_vector_database(chunks,embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_type=\"similarity_score_threshold\",search_kwargs={'score_threshold': 0.4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver_docs = retriever.invoke(\"The easiest and most mature way to store hydrogen gas is\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, clearly state that you don't know, and avoid making up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed and informative response based on the context given. If relevant, elaborate on key points to enhance understanding.\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(template=prompt_template, input_veriables=['context','question'])\n",
    "chain_type_kwargs={\"prompt\":PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Define model_kwargs\n",
    "model_kwargs = {\n",
    "    \"max_length\": 128\n",
    "}\n",
    "\n",
    "# Initialize the model with the updated parameters\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    token=\"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\",\n",
    "    temperature=0.7,  # Pass temperature explicitly\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"The easiest and most mature way to store hydrogen gas is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vector_db.as_retriever(search_kwargs={'k': 2,'score_threshold': 0.7},search_type=\"similarity_score_threshold\"),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa.invoke({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response :  \n",
      "The most mature and safest method for storing hydrogen gas is in a high-pressure cylinder made of a strong material like steel or aluminum alloy. The cylinders are designed to withstand the pressure and are equipped with safety features such as pressure relief valves and overfill protection devices to prevent over-pressurization. It's important to store hydrogen in a cool, dry, and well-ventilated area to minimize the risk of a fire or explosion. Additionally, hydrogen should be stored away from incompatible materials and sources of ignition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response :  Amal Chuhan is a fictional character from the popular Netflix series \"Mindhunter.\" She is portrayed by the actress Holt McCallany. Amal Chuhan is an FBI agent who works closely with the protagonists Holden Ford and Bill Tench, helping them in their efforts to understand the psychology of serial killers. She is known for her sharp intellect and strong determination in her work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response :  \n",
      "The most mature and safest method for storing hydrogen gas is in a high-pressure cylinder made of a strong material like steel or aluminum alloy. The cylinders are designed to withstand the pressure and are equipped with safety features such as pressure relief valves and overfill protection devices to prevent over-pressurization. It's important to store hydrogen in a cool, dry, and well-ventilated area to minimize the risk of a fire or explosion. Additionally, hydrogen should be stored away from incompatible materials and sources of ignition.\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    result=qa.invoke({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
